{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import os\n",
    "\n",
    "# Vérification du GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device utilisé :\", device)\n"
   ],
   "id": "2adcc957648463e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# Dossier racine de ton dataset\n",
    "root_dir = \"dataset\"  # dossier créé par icrawler\n",
    "\n",
    "# Transformations pour l'entrainement (avec augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformations pour la validation / test\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset complet\n",
    "full_dataset = datasets.ImageFolder(root=root_dir, transform=train_transform)\n",
    "num_classes = len(full_dataset.classes)\n",
    "print(\"Classes :\", full_dataset.classes)\n",
    "\n",
    "# Split train / val\n",
    "val_size = int(0.2 * len(full_dataset))\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Remplacer transform pour val_dataset\n",
    "val_dataset.dataset.transform = val_transform\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ],
   "id": "4097feebeb555f87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Utilisation d'un modèle pré-entraîné ResNet18\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Remplacer la dernière couche pour notre nombre de classes\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Criterion et optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ],
   "id": "ffeae265dc648858",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 30  # tu peux augmenter selon besoin\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / train_size\n",
    "    epoch_acc = running_corrects.double() / train_size\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n"
   ],
   "id": "8af936c2e3108a6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=full_dataset.classes)\n",
    "disp.plot(xticks_rotation=90)\n",
    "plt.show()\n"
   ],
   "id": "69e21a42b4de0037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Sélection d'une image de validation\n",
    "example_img, _ = val_dataset[0]\n",
    "input_tensor = example_img.unsqueeze(0).to(device)\n",
    "\n",
    "# Grad-CAM setup\n",
    "gradients = []\n",
    "activations = []\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    activations.append(output)\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    gradients.append(grad_output[0])\n",
    "\n",
    "# On hook la dernière couche conv (layer4[-1].conv2 pour ResNet18)\n",
    "target_layer = model.layer4[-1].conv2\n",
    "target_layer.register_forward_hook(forward_hook)\n",
    "target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "output = model(input_tensor)\n",
    "pred_class = output.argmax(dim=1)\n",
    "\n",
    "# Backward pass\n",
    "model.zero_grad()\n",
    "output[:, pred_class].backward()\n",
    "\n",
    "# Récupération des activations et gradients\n",
    "grad = gradients[0].cpu().data.numpy()[0]\n",
    "activation = activations[0].cpu().data.numpy()[0]\n",
    "\n",
    "# Pondération des cartes d'activation par les gradients moyens\n",
    "weights = np.mean(grad, axis=(1, 2))\n",
    "cam = np.zeros(activation.shape[1:], dtype=np.float32)\n",
    "\n",
    "for i, w in enumerate(weights):\n",
    "    cam += w * activation[i]\n",
    "\n",
    "# Normalisation\n",
    "cam = np.maximum(cam, 0)\n",
    "cam = cam / cam.max()\n",
    "\n",
    "# Conversion en image\n",
    "cam = np.uint8(cam * 255)\n",
    "cam = np.stack([cam]*3, axis=2)\n",
    "cam = F.resize(torch.tensor(cam).permute(2,0,1), [224,224])\n",
    "\n",
    "# Affichage\n",
    "plt.imshow(example_img.permute(1,2,0).cpu())  # image originale\n",
    "plt.imshow(cam.permute(1,2,0), cmap='jet', alpha=0.5)  # superposé\n",
    "plt.title(f\"Grad-CAM pour la classe: {full_dataset.classes[pred_class]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ],
   "id": "8cef52486f259582",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
